"""
ç»¼åˆæµ‹è¯•è¿è¡Œå™¨
æ‰§è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶å¹¶ç”ŸæˆæŠ¥å‘Š
"""
import pytest
import sys
import os
import time
import json
from pathlib import Path
from typing import Dict, List, Any
import subprocess
import psutil


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.results = {
            "unit_tests": {},
            "integration_tests": {},
            "performance_tests": {},
            "coverage": {},
            "summary": {}
        }
        self.start_time = time.time()
    
    def run_unit_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        print("ğŸ§ª è¿è¡Œå•å…ƒæµ‹è¯•...")
        
        # è¿è¡Œå•å…ƒæµ‹è¯•
        result = pytest.main([
            "tests/",
            "--ignore=tests/integration/",
            "--ignore=tests/performance/",
            "-v",
            "--tb=short",
            "--cov=app",
            "--cov-report=json",
            "--cov-report=term-missing",
            "--junit-xml=test-results/unit-tests.xml"
        ])
        
        # è¯»å–è¦†ç›–ç‡æŠ¥å‘Š
        coverage_data = {}
        try:
            with open("coverage.json", "r") as f:
                coverage_data = json.load(f)
        except FileNotFoundError:
            pass
        
        return {
            "exit_code": result,
            "coverage": coverage_data,
            "passed": result == 0
        }
    
    def run_integration_tests(self) -> Dict[str, Any]:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        print("ğŸ”— è¿è¡Œé›†æˆæµ‹è¯•...")
        
        # ç¡®ä¿æµ‹è¯•æ•°æ®åº“å­˜åœ¨
        self._setup_test_database()
        
        result = pytest.main([
            "tests/integration/",
            "-v",
            "--tb=short",
            "--junit-xml=test-results/integration-tests.xml"
        ])
        
        return {
            "exit_code": result,
            "passed": result == 0
        }
    
    def run_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print("âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•...")
        
        # è®°å½•ç³»ç»Ÿèµ„æºä½¿ç”¨
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        initial_cpu = process.cpu_percent()
        
        result = pytest.main([
            "tests/performance/",
            "-v",
            "--tb=short",
            "-s",  # æ˜¾ç¤ºprintè¾“å‡º
            "--junit-xml=test-results/performance-tests.xml"
        ])
        
        final_memory = process.memory_info().rss / 1024 / 1024
        final_cpu = process.cpu_percent()
        
        return {
            "exit_code": result,
            "passed": result == 0,
            "resource_usage": {
                "memory_start_mb": initial_memory,
                "memory_end_mb": final_memory,
                "memory_delta_mb": final_memory - initial_memory,
                "cpu_start_percent": initial_cpu,
                "cpu_end_percent": final_cpu
            }
        }
    
    def run_frontend_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå‰ç«¯æµ‹è¯•"""
        print("ğŸ¨ è¿è¡Œå‰ç«¯æµ‹è¯•...")
        
        # åˆ‡æ¢åˆ°å‰ç«¯ç›®å½•
        frontend_dir = Path(__file__).parent.parent.parent / "frontend"
        
        try:
            # è¿è¡Œå‰ç«¯æµ‹è¯•
            result = subprocess.run([
                "npm", "run", "test", "--", "--run", "--reporter=json"
            ], 
            cwd=frontend_dir,
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            # è§£ææµ‹è¯•ç»“æœ
            test_output = {}
            if result.stdout:
                try:
                    test_output = json.loads(result.stdout)
                except json.JSONDecodeError:
                    test_output = {"raw_output": result.stdout}
            
            return {
                "exit_code": result.returncode,
                "passed": result.returncode == 0,
                "output": test_output,
                "stderr": result.stderr
            }
            
        except subprocess.TimeoutExpired:
            return {
                "exit_code": -1,
                "passed": False,
                "error": "Frontend tests timed out"
            }
        except Exception as e:
            return {
                "exit_code": -1,
                "passed": False,
                "error": str(e)
            }
    
    def _setup_test_database(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®åº“"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®åº“ç›®å½•
        os.makedirs("test-results", exist_ok=True)
        
        # æ¸…ç†æ—§çš„æµ‹è¯•æ•°æ®åº“
        test_db_files = ["test.db", "test_integration.db"]
        for db_file in test_db_files:
            if os.path.exists(db_file):
                os.remove(db_file)
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        all_passed = all([
            self.results["unit_tests"].get("passed", False),
            self.results["integration_tests"].get("passed", False),
            self.results["performance_tests"].get("passed", False),
            self.results["frontend_tests"].get("passed", False)
        ])
        
        summary = {
            "total_duration_seconds": total_time,
            "all_tests_passed": all_passed,
            "test_suites": {
                "unit_tests": self.results["unit_tests"].get("passed", False),
                "integration_tests": self.results["integration_tests"].get("passed", False),
                "performance_tests": self.results["performance_tests"].get("passed", False),
                "frontend_tests": self.results["frontend_tests"].get("passed", False)
            },
            "coverage": self.results["unit_tests"].get("coverage", {}),
            "performance_metrics": self.results["performance_tests"].get("resource_usage", {}),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # ä¿å­˜æŠ¥å‘Š
        os.makedirs("test-results", exist_ok=True)
        with open("test-results/test-report.json", "w", encoding="utf-8") as f:
            json.dump({
                "summary": summary,
                "detailed_results": self.results
            }, f, indent=2, ensure_ascii=False)
        
        return summary
    
    def print_summary(self, summary: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•æ‰§è¡Œæ‘˜è¦")
        print("="*60)
        
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {summary['total_duration_seconds']:.2f}ç§’")
        print(f"âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡: {'æ˜¯' if summary['all_tests_passed'] else 'å¦'}")
        
        print("\nğŸ“‹ æµ‹è¯•å¥—ä»¶ç»“æœ:")
        for suite_name, passed in summary["test_suites"].items():
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            print(f"  {suite_name}: {status}")
        
        # è¦†ç›–ç‡ä¿¡æ¯
        coverage = summary.get("coverage", {})
        if coverage and "totals" in coverage:
            total_coverage = coverage["totals"].get("percent_covered", 0)
            print(f"\nğŸ“ˆ ä»£ç è¦†ç›–ç‡: {total_coverage:.1f}%")
        
        # æ€§èƒ½æŒ‡æ ‡
        perf_metrics = summary.get("performance_metrics", {})
        if perf_metrics:
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            print(f"  å†…å­˜ä½¿ç”¨å˜åŒ–: {perf_metrics.get('memory_delta_mb', 0):.1f}MB")
        
        print("\n" + "="*60)
        
        if not summary['all_tests_passed']:
            print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¯¦ç»†æ—¥å¿—")
            return 1
        else:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            return 0
    
    def run_all_tests(self) -> int:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶...")
        
        # è¿è¡Œå„ç±»æµ‹è¯•
        self.results["unit_tests"] = self.run_unit_tests()
        self.results["integration_tests"] = self.run_integration_tests()
        self.results["performance_tests"] = self.run_performance_tests()
        self.results["frontend_tests"] = self.run_frontend_tests()
        
        # ç”ŸæˆæŠ¥å‘Š
        summary = self.generate_report()
        
        # æ‰“å°æ‘˜è¦
        return self.print_summary(summary)


def main():
    """ä¸»å‡½æ•°"""
    runner = TestRunner()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        test_type = sys.argv[1]
        
        if test_type == "unit":
            result = runner.run_unit_tests()
            return 0 if result["passed"] else 1
        elif test_type == "integration":
            result = runner.run_integration_tests()
            return 0 if result["passed"] else 1
        elif test_type == "performance":
            result = runner.run_performance_tests()
            return 0 if result["passed"] else 1
        elif test_type == "frontend":
            result = runner.run_frontend_tests()
            return 0 if result["passed"] else 1
        else:
            print(f"æœªçŸ¥çš„æµ‹è¯•ç±»å‹: {test_type}")
            print("å¯ç”¨é€‰é¡¹: unit, integration, performance, frontend")
            return 1
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    return runner.run_all_tests()


if __name__ == "__main__":
    sys.exit(main())