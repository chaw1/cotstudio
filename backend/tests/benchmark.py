"""
COT Studio MVP æ€§èƒ½åŸºå‡†æµ‹è¯•
æ‰§è¡Œç³»ç»Ÿæ€§èƒ½æµ‹è¯•å¹¶ç”ŸæˆåŸºå‡†æŠ¥å‘Š
"""
import time
import json
import statistics
import psutil
import asyncio
from typing import Dict, List, Any, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict

import pytest
from fastapi.testclient import TestClient

from app.main import app


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    name: str
    value: float
    unit: str
    threshold: float
    passed: bool
    details: Dict[str, Any] = None


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    duration: float
    metrics: List[PerformanceMetric]
    resource_usage: Dict[str, float]
    passed: bool
    error: str = None


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.client = TestClient(app)
        self.results: List[BenchmarkResult] = []
        self.process = psutil.Process()
    
    def measure_resource_usage(self, func: Callable) -> Dict[str, Any]:
        """æµ‹é‡èµ„æºä½¿ç”¨æƒ…å†µ"""
        # è®°å½•åˆå§‹çŠ¶æ€
        initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        initial_cpu = self.process.cpu_percent()
        
        start_time = time.time()
        
        # æ‰§è¡Œå‡½æ•°
        try:
            result = func()
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)
        
        end_time = time.time()
        
        # è®°å½•æœ€ç»ˆçŠ¶æ€
        final_memory = self.process.memory_info().rss / 1024 / 1024
        final_cpu = self.process.cpu_percent()
        
        return {
            "result": result,
            "success": success,
            "error": error,
            "duration": end_time - start_time,
            "memory_start_mb": initial_memory,
            "memory_end_mb": final_memory,
            "memory_delta_mb": final_memory - initial_memory,
            "cpu_start_percent": initial_cpu,
            "cpu_end_percent": final_cpu
        }
    
    def benchmark_api_response_times(self) -> BenchmarkResult:
        """APIå“åº”æ—¶é—´åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ æ‰§è¡ŒAPIå“åº”æ—¶é—´åŸºå‡†æµ‹è¯•...")
        
        # æµ‹è¯•ä¸åŒAPIç«¯ç‚¹çš„å“åº”æ—¶é—´
        endpoints = [
            ("GET", "/health", None),
            ("GET", "/api/v1/info", None),
            ("POST", "/api/v1/projects/", {"name": "Benchmark Test", "description": "Performance test"}),
        ]
        
        response_times = []
        metrics = []
        
        def test_endpoint(method: str, url: str, data: Dict = None):
            start = time.time()
            
            if method == "GET":
                response = self.client.get(url)
            elif method == "POST":
                response = self.client.post(url, json=data)
            
            duration = time.time() - start
            return duration, response.status_code
        
        # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•è·å–ç»Ÿè®¡æ•°æ®
        for method, url, data in endpoints:
            endpoint_times = []
            
            for _ in range(10):  # æ¯ä¸ªç«¯ç‚¹æµ‹è¯•10æ¬¡
                try:
                    duration, status_code = test_endpoint(method, url, data)
                    if status_code < 400:  # åªè®°å½•æˆåŠŸçš„è¯·æ±‚
                        endpoint_times.append(duration)
                except Exception:
                    continue
            
            if endpoint_times:
                avg_time = statistics.mean(endpoint_times)
                p95_time = statistics.quantiles(endpoint_times, n=20)[18] if len(endpoint_times) >= 20 else max(endpoint_times)
                
                response_times.extend(endpoint_times)
                
                # åˆ›å»ºæŒ‡æ ‡
                metrics.append(PerformanceMetric(
                    name=f"{method} {url} - Average Response Time",
                    value=avg_time,
                    unit="seconds",
                    threshold=2.0,  # 2ç§’é˜ˆå€¼
                    passed=avg_time < 2.0,
                    details={"p95": p95_time, "samples": len(endpoint_times)}
                ))
        
        # æ•´ä½“ç»Ÿè®¡
        if response_times:
            overall_avg = statistics.mean(response_times)
            overall_p95 = statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times)
            
            metrics.append(PerformanceMetric(
                name="Overall Average Response Time",
                value=overall_avg,
                unit="seconds", 
                threshold=1.0,
                passed=overall_avg < 1.0
            ))
            
            metrics.append(PerformanceMetric(
                name="Overall P95 Response Time",
                value=overall_p95,
                unit="seconds",
                threshold=2.0,
                passed=overall_p95 < 2.0
            ))
        
        # èµ„æºä½¿ç”¨æƒ…å†µ
        resource_usage = {
            "memory_mb": self.process.memory_info().rss / 1024 / 1024,
            "cpu_percent": self.process.cpu_percent()
        }
        
        all_passed = all(m.passed for m in metrics)
        
        return BenchmarkResult(
            test_name="API Response Times",
            duration=sum(response_times) if response_times else 0,
            metrics=metrics,
            resource_usage=resource_usage,
            passed=all_passed
        )
    
    def benchmark_concurrent_requests(self) -> BenchmarkResult:
        """å¹¶å‘è¯·æ±‚åŸºå‡†æµ‹è¯•"""
        print("ğŸ”„ æ‰§è¡Œå¹¶å‘è¯·æ±‚åŸºå‡†æµ‹è¯•...")
        
        def make_request():
            """å•ä¸ªè¯·æ±‚"""
            start = time.time()
            response = self.client.get("/health")
            duration = time.time() - start
            return duration, response.status_code
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [1, 5, 10]
        metrics = []
        
        for concurrency in concurrency_levels:
            print(f"  æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
            
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = [executor.submit(make_request) for _ in range(concurrency * 2)]
                results = [future.result() for future in as_completed(futures)]
            
            total_time = time.time() - start_time
            
            # åˆ†æç»“æœ
            successful_requests = [r for r in results if r[1] < 400]
            if successful_requests:
                response_times = [r[0] for r in successful_requests]
                avg_response_time = statistics.mean(response_times)
                throughput = len(successful_requests) / total_time
                
                metrics.append(PerformanceMetric(
                    name=f"Concurrency {concurrency} - Average Response Time",
                    value=avg_response_time,
                    unit="seconds",
                    threshold=3.0,
                    passed=avg_response_time < 3.0
                ))
                
                metrics.append(PerformanceMetric(
                    name=f"Concurrency {concurrency} - Throughput",
                    value=throughput,
                    unit="requests/second",
                    threshold=1.0,
                    passed=throughput > 1.0
                ))
        
        resource_usage = {
            "memory_mb": self.process.memory_info().rss / 1024 / 1024,
            "cpu_percent": self.process.cpu_percent()
        }
        
        all_passed = all(m.passed for m in metrics)
        
        return BenchmarkResult(
            test_name="Concurrent Requests",
            duration=0,  # æ€»æ—¶é—´åœ¨å„ä¸ªæµ‹è¯•ä¸­å·²è®¡ç®—
            metrics=metrics,
            resource_usage=resource_usage,
            passed=all_passed
        )
    
    def benchmark_memory_usage(self) -> BenchmarkResult:
        """å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""
        print("ğŸ’¾ æ‰§è¡Œå†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•...")
        
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        
        # æ‰§è¡Œä¸€ç³»åˆ—æ“ä½œæ¥æµ‹è¯•å†…å­˜ä½¿ç”¨
        operations = []
        
        # åˆ›å»ºå¤šä¸ªé¡¹ç›®
        for i in range(10):
            project_data = {
                "name": f"Memory Test Project {i}",
                "description": f"Memory benchmark test project {i}"
            }
            
            try:
                response = self.client.post("/api/v1/projects/", json=project_data)
                if response.status_code == 201:
                    operations.append("project_created")
            except Exception:
                operations.append("project_failed")
        
        # æŸ¥è¯¢é¡¹ç›®åˆ—è¡¨å¤šæ¬¡
        for _ in range(20):
            try:
                response = self.client.get("/api/v1/projects/")
                if response.status_code == 200:
                    operations.append("list_success")
            except Exception:
                operations.append("list_failed")
        
        final_memory = self.process.memory_info().rss / 1024 / 1024
        memory_increase = final_memory - initial_memory
        
        metrics = [
            PerformanceMetric(
                name="Memory Usage Increase",
                value=memory_increase,
                unit="MB",
                threshold=100.0,  # ä¸åº”å¢é•¿è¶…è¿‡100MB
                passed=memory_increase < 100.0,
                details={
                    "initial_mb": initial_memory,
                    "final_mb": final_memory,
                    "operations": len(operations)
                }
            ),
            PerformanceMetric(
                name="Memory per Operation",
                value=memory_increase / len(operations) if operations else 0,
                unit="MB/operation",
                threshold=5.0,
                passed=(memory_increase / len(operations) if operations else 0) < 5.0
            )
        ]
        
        resource_usage = {
            "memory_initial_mb": initial_memory,
            "memory_final_mb": final_memory,
            "memory_delta_mb": memory_increase,
            "operations_count": len(operations)
        }
        
        all_passed = all(m.passed for m in metrics)
        
        return BenchmarkResult(
            test_name="Memory Usage",
            duration=0,
            metrics=metrics,
            resource_usage=resource_usage,
            passed=all_passed
        )
    
    def benchmark_database_operations(self) -> BenchmarkResult:
        """æ•°æ®åº“æ“ä½œåŸºå‡†æµ‹è¯•"""
        print("ğŸ—„ï¸ æ‰§è¡Œæ•°æ®åº“æ“ä½œåŸºå‡†æµ‹è¯•...")
        
        metrics = []
        
        # æµ‹è¯•æ‰¹é‡æ’å…¥æ€§èƒ½
        start_time = time.time()
        
        created_projects = []
        for i in range(50):  # åˆ›å»º50ä¸ªé¡¹ç›®
            project_data = {
                "name": f"DB Benchmark Project {i}",
                "description": f"Database performance test {i}",
                "tags": [f"tag{i}", "benchmark"]
            }
            
            try:
                response = self.client.post("/api/v1/projects/", json=project_data)
                if response.status_code == 201:
                    created_projects.append(response.json()["id"])
            except Exception:
                continue
        
        insert_time = time.time() - start_time
        
        if created_projects:
            avg_insert_time = insert_time / len(created_projects)
            
            metrics.append(PerformanceMetric(
                name="Average Project Creation Time",
                value=avg_insert_time,
                unit="seconds",
                threshold=0.5,
                passed=avg_insert_time < 0.5,
                details={"total_created": len(created_projects)}
            ))
        
        # æµ‹è¯•æ‰¹é‡æŸ¥è¯¢æ€§èƒ½
        start_time = time.time()
        
        successful_queries = 0
        for project_id in created_projects[:10]:  # æŸ¥è¯¢å‰10ä¸ªé¡¹ç›®
            try:
                response = self.client.get(f"/api/v1/projects/{project_id}")
                if response.status_code == 200:
                    successful_queries += 1
            except Exception:
                continue
        
        query_time = time.time() - start_time
        
        if successful_queries > 0:
            avg_query_time = query_time / successful_queries
            
            metrics.append(PerformanceMetric(
                name="Average Project Query Time",
                value=avg_query_time,
                unit="seconds",
                threshold=0.1,
                passed=avg_query_time < 0.1,
                details={"queries_executed": successful_queries}
            ))
        
        resource_usage = {
            "memory_mb": self.process.memory_info().rss / 1024 / 1024,
            "cpu_percent": self.process.cpu_percent()
        }
        
        all_passed = all(m.passed for m in metrics)
        
        return BenchmarkResult(
            test_name="Database Operations",
            duration=insert_time + query_time,
            metrics=metrics,
            resource_usage=resource_usage,
            passed=all_passed
        )
    
    def run_all_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("ğŸ¯ å¼€å§‹æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶...")
        print("=" * 60)
        
        benchmarks = [
            self.benchmark_api_response_times,
            self.benchmark_concurrent_requests,
            self.benchmark_memory_usage,
            self.benchmark_database_operations
        ]
        
        start_time = time.time()
        
        for benchmark_func in benchmarks:
            try:
                result = benchmark_func()
                self.results.append(result)
                
                # æ‰“å°å³æ—¶ç»“æœ
                status = "âœ… é€šè¿‡" if result.passed else "âŒ å¤±è´¥"
                print(f"{result.test_name}: {status}")
                
                for metric in result.metrics:
                    metric_status = "âœ…" if metric.passed else "âŒ"
                    print(f"  {metric_status} {metric.name}: {metric.value:.3f} {metric.unit}")
                
                print()
                
            except Exception as e:
                error_result = BenchmarkResult(
                    test_name=benchmark_func.__name__,
                    duration=0,
                    metrics=[],
                    resource_usage={},
                    passed=False,
                    error=str(e)
                )
                self.results.append(error_result)
                print(f"âŒ {benchmark_func.__name__}: æ‰§è¡Œå¤±è´¥ - {e}")
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        return self.generate_report(total_time)
    
    def generate_report(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½åŸºå‡†æŠ¥å‘Š"""
        all_passed = all(result.passed for result in self.results)
        
        # ç»Ÿè®¡æŒ‡æ ‡
        total_metrics = sum(len(result.metrics) for result in self.results)
        passed_metrics = sum(
            len([m for m in result.metrics if m.passed]) 
            for result in self.results
        )
        
        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        pass_rate = passed_metrics / total_metrics if total_metrics > 0 else 0
        if pass_rate >= 0.9:
            performance_grade = "A"
        elif pass_rate >= 0.8:
            performance_grade = "B"
        elif pass_rate >= 0.7:
            performance_grade = "C"
        else:
            performance_grade = "D"
        
        report = {
            "summary": {
                "total_duration_seconds": total_time,
                "all_benchmarks_passed": all_passed,
                "performance_grade": performance_grade,
                "pass_rate": pass_rate,
                "total_metrics": total_metrics,
                "passed_metrics": passed_metrics,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            "benchmarks": [asdict(result) for result in self.results],
            "system_info": {
                "memory_mb": self.process.memory_info().rss / 1024 / 1024,
                "cpu_count": psutil.cpu_count(),
                "cpu_percent": self.process.cpu_percent()
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path("test-results/benchmark-report.json")
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report
    
    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°åŸºå‡†æµ‹è¯•æ‘˜è¦"""
        summary = report["summary"]
        
        print("=" * 60)
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æ‘˜è¦")
        print("=" * 60)
        
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {summary['total_duration_seconds']:.2f}ç§’")
        print(f"âœ… æ‰€æœ‰åŸºå‡†é€šè¿‡: {'æ˜¯' if summary['all_benchmarks_passed'] else 'å¦'}")
        print(f"ğŸ“ˆ é€šè¿‡ç‡: {summary['pass_rate']:.1%}")
        print(f"â­ æ€§èƒ½ç­‰çº§: {summary['performance_grade']}")
        
        print(f"\nğŸ“‹ åŸºå‡†æµ‹è¯•ç»“æœ:")
        for result in self.results:
            status = "âœ… é€šè¿‡" if result.passed else "âŒ å¤±è´¥"
            print(f"  {result.test_name}: {status}")
        
        print(f"\nğŸ’» ç³»ç»Ÿä¿¡æ¯:")
        sys_info = report["system_info"]
        print(f"  å†…å­˜ä½¿ç”¨: {sys_info['memory_mb']:.1f}MB")
        print(f"  CPUæ ¸å¿ƒæ•°: {sys_info['cpu_count']}")
        print(f"  CPUä½¿ç”¨ç‡: {sys_info['cpu_percent']:.1f}%")
        
        print("\n" + "=" * 60)
        
        if summary['all_benchmarks_passed']:
            print("ğŸ‰ æ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡ï¼")
        else:
            print("âš ï¸  éƒ¨åˆ†æ€§èƒ½åŸºå‡†æœªè¾¾æ ‡ï¼Œè¯·æ£€æŸ¥è¯¦ç»†æŠ¥å‘Šã€‚")
        
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: test-results/benchmark-report.json")


def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    
    try:
        report = benchmark.run_all_benchmarks()
        benchmark.print_summary(report)
        
        return 0 if report["summary"]["all_benchmarks_passed"] else 1
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())