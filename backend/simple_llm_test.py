"""
ç®€å•çš„LLMæµ‹è¯•è„šæœ¬
"""
import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from openai import AsyncOpenAI
    print("âœ“ OpenAIåº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âœ— OpenAIåº“å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


async def test_deepseek_direct():
    """ç›´æ¥æµ‹è¯•DeepSeek API"""
    print("=== ç›´æ¥æµ‹è¯•DeepSeek API ===")
    
    # DeepSeeké…ç½®
    api_key = "sk-0dc1980d2c264b19bde7da0c209e13dd"
    base_url = "https://api.deepseek.com"
    model = "deepseek-chat"
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=60
        )
        
        print(f"âœ“ å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        print(f"  API Key: {api_key[:10]}...")
        print(f"  Base URL: {base_url}")
        print(f"  Model: {model}")
        
        # å‘é€æµ‹è¯•è¯·æ±‚
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ ã€‚"}
        ]
        
        print("å‘é€è¯·æ±‚...")
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.7,
            max_tokens=200
        )
        
        print("âœ“ è¯·æ±‚æˆåŠŸ")
        print(f"å“åº”å†…å®¹: {response.choices[0].message.content}")
        
        if response.usage:
            print(f"Tokenä½¿ç”¨: {response.usage.model_dump()}")
        
        await client.close()
        return True
        
    except Exception as e:
        print(f"âœ— è¯·æ±‚å¤±è´¥: {str(e)}")
        return False


async def test_cot_prompt():
    """æµ‹è¯•CoTæç¤ºè¯"""
    print("\n=== æµ‹è¯•CoTæç¤ºè¯ ===")
    
    api_key = "sk-0dc1980d2c264b19bde7da0c209e13dd"
    base_url = "https://api.deepseek.com"
    model = "deepseek-chat"
    
    try:
        client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=60
        )
        
        # CoTé—®é¢˜ç”Ÿæˆæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºç»™å®šæ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡çš„Chain-of-Thought (CoT)é—®é¢˜ã€‚

è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µï¼Œç”Ÿæˆä¸€ä¸ªéœ€è¦æ·±åº¦æ€è€ƒå’Œæ¨ç†çš„å­¦æœ¯çº§åˆ«é—®é¢˜ã€‚é—®é¢˜åº”è¯¥ï¼š
1. éœ€è¦å¤šæ­¥æ¨ç†æ‰èƒ½å›ç­”
2. å…·æœ‰å­¦æœ¯ä»·å€¼å’Œæ·±åº¦
3. èƒ½å¤Ÿå¼•å¯¼å‡ºæ¸…æ™°çš„æ€ç»´é“¾æ¡
4. é€‚åˆè¿›è¡ŒCoTæ ¼å¼çš„å›ç­”

è¯·ç›´æ¥è¿”å›é—®é¢˜ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚é—®é¢˜é•¿åº¦æ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚"""

        user_content = """æ–‡æœ¬ç‰‡æ®µï¼š
æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚
æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†æå¤§é‡æ•°æ®æ¥è¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼æ¥å¯¹æ–°æ•°æ®åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚
å¸¸è§çš„æœºå™¨å­¦ä¹ ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]
        
        print("ç”ŸæˆCoTé—®é¢˜...")
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.8,
            max_tokens=200
        )
        
        question = response.choices[0].message.content.strip()
        print(f"âœ“ ç”Ÿæˆçš„é—®é¢˜: {question}")
        
        # æµ‹è¯•å€™é€‰ç­”æ¡ˆç”Ÿæˆ
        answer_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„Chain-of-Thought (CoT)æ ¼å¼ç­”æ¡ˆã€‚

è¯·åŸºäºç»™å®šçš„é—®é¢˜å’Œæ–‡æœ¬ç‰‡æ®µï¼Œç”Ÿæˆ3ä¸ªä¸åŒçš„CoTæ ¼å¼ç­”æ¡ˆã€‚æ¯ä¸ªç­”æ¡ˆéƒ½åº”è¯¥ï¼š
1. åŒ…å«æ¸…æ™°çš„æ€ç»´æ­¥éª¤
2. é€»è¾‘æ¨ç†è¿‡ç¨‹å®Œæ•´
3. ç»“è®ºæ˜ç¡®ä¸”æœ‰ä¾æ®
4. é£æ ¼å’Œè§’åº¦ç•¥æœ‰ä¸åŒ

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
    "candidates": [
        {{
            "text": "æœ€ç»ˆç­”æ¡ˆå†…å®¹",
            "chain_of_thought": "è¯¦ç»†çš„æ€ç»´æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…å«æ­¥éª¤1ã€æ­¥éª¤2ç­‰"
        }},
        {{
            "text": "æœ€ç»ˆç­”æ¡ˆå†…å®¹",
            "chain_of_thought": "è¯¦ç»†çš„æ€ç»´æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…å«æ­¥éª¤1ã€æ­¥éª¤2ç­‰"
        }},
        {{
            "text": "æœ€ç»ˆç­”æ¡ˆå†…å®¹", 
            "chain_of_thought": "è¯¦ç»†çš„æ€ç»´æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…å«æ­¥éª¤1ã€æ­¥éª¤2ç­‰"
        }}
    ]
}}

ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œæ¯ä¸ªç­”æ¡ˆéƒ½æœ‰å®Œæ•´çš„æ€ç»´é“¾æ¡ã€‚

é—®é¢˜ï¼š{question}

æ–‡æœ¬ç‰‡æ®µï¼š
{user_content.replace('æ–‡æœ¬ç‰‡æ®µï¼š', '').strip()}"""

        answer_messages = [
            {"role": "user", "content": answer_prompt}
        ]
        
        print("ç”Ÿæˆå€™é€‰ç­”æ¡ˆ...")
        answer_response = await client.chat.completions.create(
            model=model,
            messages=answer_messages,
            temperature=0.9,
            max_tokens=1500
        )
        
        answer_content = answer_response.choices[0].message.content
        print(f"âœ“ ç”Ÿæˆçš„å€™é€‰ç­”æ¡ˆ: {answer_content[:200]}...")
        
        await client.close()
        return True
        
    except Exception as e:
        print(f"âœ— CoTæç¤ºè¯æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹LLMé›†æˆæµ‹è¯•...")
    
    # è¿è¡Œæµ‹è¯•
    test1_result = await test_deepseek_direct()
    test2_result = await test_cot_prompt()
    
    print("\n=== æµ‹è¯•ç»“æœ ===")
    print(f"DeepSeekç›´æ¥æµ‹è¯•: {'âœ“ é€šè¿‡' if test1_result else 'âœ— å¤±è´¥'}")
    print(f"CoTæç¤ºè¯æµ‹è¯•: {'âœ“ é€šè¿‡' if test2_result else 'âœ— å¤±è´¥'}")
    
    if test1_result and test2_result:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMé›†æˆå‡†å¤‡å°±ç»ªã€‚")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")


if __name__ == "__main__":
    asyncio.run(main())