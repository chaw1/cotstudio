#!/usr/bin/env python3
"""
æµ‹è¯•éªŒè¯è„šæœ¬
éªŒè¯æµ‹è¯•å¥—ä»¶çš„å®Œæ•´æ€§å’Œå¯æ‰§è¡Œæ€§
"""
import os
import sys
import subprocess
import json
from pathlib import Path


def check_file_exists(file_path: str, description: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    if os.path.exists(file_path):
        print(f"âœ… {description}: {file_path}")
        return True
    else:
        print(f"âŒ {description}: {file_path} (ä¸å­˜åœ¨)")
        return False


def check_python_imports() -> bool:
    """æ£€æŸ¥Pythonä¾èµ–æ˜¯å¦å¯å¯¼å…¥"""
    required_modules = [
        'pytest',
        'fastapi',
        'sqlalchemy',
        'psutil'
    ]
    
    success = True
    for module in required_modules:
        try:
            __import__(module)
            print(f"âœ… Pythonæ¨¡å—: {module}")
        except ImportError:
            print(f"âŒ Pythonæ¨¡å—: {module} (æœªå®‰è£…)")
            success = False
    
    return success


def validate_test_structure() -> bool:
    """éªŒè¯æµ‹è¯•ç›®å½•ç»“æ„"""
    print("\nğŸ” éªŒè¯æµ‹è¯•ç›®å½•ç»“æ„...")
    
    required_files = [
        # åç«¯æµ‹è¯•æ–‡ä»¶
        ("backend/tests/__init__.py", "åç«¯æµ‹è¯•åŒ…"),
        ("backend/tests/conftest.py", "pytesté…ç½®"),
        ("backend/tests/integration/__init__.py", "é›†æˆæµ‹è¯•åŒ…"),
        ("backend/tests/integration/test_complete_workflow.py", "å®Œæ•´æµç¨‹æµ‹è¯•"),
        ("backend/tests/integration/test_knowledge_graph_integration.py", "çŸ¥è¯†å›¾è°±é›†æˆæµ‹è¯•"),
        ("backend/tests/performance/__init__.py", "æ€§èƒ½æµ‹è¯•åŒ…"),
        ("backend/tests/performance/test_load_performance.py", "è´Ÿè½½æ€§èƒ½æµ‹è¯•"),
        ("backend/tests/test_runner.py", "æµ‹è¯•è¿è¡Œå™¨"),
        ("backend/tests/benchmark.py", "æ€§èƒ½åŸºå‡†æµ‹è¯•"),
        ("backend/pytest.ini", "pytesté…ç½®æ–‡ä»¶"),
        
        # å‰ç«¯æµ‹è¯•æ–‡ä»¶
        ("frontend/src/test/setup.ts", "å‰ç«¯æµ‹è¯•è®¾ç½®"),
        ("frontend/src/test/e2e/complete-workflow.test.tsx", "å‰ç«¯E2Eæµ‹è¯•"),
        ("frontend/src/test/integration/knowledge-graph.test.tsx", "å‰ç«¯KGé›†æˆæµ‹è¯•"),
        ("frontend/vitest.config.ts", "Vitesté…ç½®"),
        
        # æ–‡æ¡£
        ("test-documentation.md", "æµ‹è¯•æ–‡æ¡£"),
    ]
    
    success = True
    for file_path, description in required_files:
        if not check_file_exists(file_path, description):
            success = False
    
    return success


def validate_test_content() -> bool:
    """éªŒè¯æµ‹è¯•æ–‡ä»¶å†…å®¹"""
    print("\nğŸ” éªŒè¯æµ‹è¯•æ–‡ä»¶å†…å®¹...")
    
    # æ£€æŸ¥åç«¯æµ‹è¯•æ–‡ä»¶
    backend_test_file = "backend/tests/integration/test_complete_workflow.py"
    if os.path.exists(backend_test_file):
        with open(backend_test_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
        required_patterns = [
            "class TestCompleteWorkflow",
            "test_complete_file_to_cot_workflow",
            "test_knowledge_graph_extraction_workflow",
            "test_export_import_workflow",
            "test_error_handling_and_recovery"
        ]
        
        for pattern in required_patterns:
            if pattern in content:
                print(f"âœ… åç«¯æµ‹è¯•åŒ…å«: {pattern}")
            else:
                print(f"âŒ åç«¯æµ‹è¯•ç¼ºå°‘: {pattern}")
                return False
    
    # æ£€æŸ¥å‰ç«¯æµ‹è¯•æ–‡ä»¶
    frontend_test_file = "frontend/src/test/e2e/complete-workflow.test.tsx"
    if os.path.exists(frontend_test_file):
        with open(frontend_test_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
        required_patterns = [
            "Complete Workflow E2E Tests",
            "should complete full project creation to CoT annotation workflow",
            "should handle file upload errors gracefully",
            "should handle OCR processing failures"
        ]
        
        for pattern in required_patterns:
            if pattern in content:
                print(f"âœ… å‰ç«¯æµ‹è¯•åŒ…å«: {pattern}")
            else:
                print(f"âŒ å‰ç«¯æµ‹è¯•ç¼ºå°‘: {pattern}")
                return False
    
    return True


def check_test_requirements() -> bool:
    """æ£€æŸ¥æµ‹è¯•éœ€æ±‚è¦†ç›–"""
    print("\nğŸ” æ£€æŸ¥æµ‹è¯•éœ€æ±‚è¦†ç›–...")
    
    # æ ¹æ®ä»»åŠ¡è¦æ±‚æ£€æŸ¥æµ‹è¯•è¦†ç›–
    required_test_areas = [
        "ç«¯åˆ°ç«¯æµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–å®Œæ•´ä¸šåŠ¡æµç¨‹",
        "æ–‡ä»¶ä¸Šä¼ åˆ°CoTç”Ÿæˆçš„å®Œæ•´æµç¨‹æµ‹è¯•", 
        "çŸ¥è¯†å›¾è°±æŠ½å–å’Œå¯è§†åŒ–çš„é›†æˆæµ‹è¯•",
        "æ€§èƒ½æµ‹è¯•å’Œè´Ÿè½½æµ‹è¯•"
    ]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æµ‹è¯•å®ç°
    test_implementations = {
        "ç«¯åˆ°ç«¯æµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–å®Œæ•´ä¸šåŠ¡æµç¨‹": [
            "frontend/src/test/e2e/complete-workflow.test.tsx",
            "backend/tests/integration/test_complete_workflow.py"
        ],
        "æ–‡ä»¶ä¸Šä¼ åˆ°CoTç”Ÿæˆçš„å®Œæ•´æµç¨‹æµ‹è¯•": [
            "backend/tests/integration/test_complete_workflow.py"
        ],
        "çŸ¥è¯†å›¾è°±æŠ½å–å’Œå¯è§†åŒ–çš„é›†æˆæµ‹è¯•": [
            "backend/tests/integration/test_knowledge_graph_integration.py",
            "frontend/src/test/integration/knowledge-graph.test.tsx"
        ],
        "æ€§èƒ½æµ‹è¯•å’Œè´Ÿè½½æµ‹è¯•": [
            "backend/tests/performance/test_load_performance.py",
            "backend/tests/benchmark.py"
        ]
    }
    
    success = True
    for requirement in required_test_areas:
        print(f"\nğŸ“‹ éœ€æ±‚: {requirement}")
        if requirement in test_implementations:
            for impl_file in test_implementations[requirement]:
                if os.path.exists(impl_file):
                    print(f"  âœ… å®ç°: {impl_file}")
                else:
                    print(f"  âŒ ç¼ºå°‘: {impl_file}")
                    success = False
        else:
            print(f"  âŒ æœªæ‰¾åˆ°å¯¹åº”å®ç°")
            success = False
    
    return success


def generate_test_summary() -> dict:
    """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
    summary = {
        "timestamp": "2024-01-01 00:00:00",
        "test_files": {
            "backend_unit_tests": len([f for f in os.listdir("backend/tests") if f.startswith("test_") and f.endswith(".py")]) if os.path.exists("backend/tests") else 0,
            "backend_integration_tests": len([f for f in os.listdir("backend/tests/integration") if f.endswith(".py")]) if os.path.exists("backend/tests/integration") else 0,
            "backend_performance_tests": len([f for f in os.listdir("backend/tests/performance") if f.endswith(".py")]) if os.path.exists("backend/tests/performance") else 0,
            "frontend_tests": len([f for f in Path("frontend/src/test").rglob("*.test.tsx")]) if os.path.exists("frontend/src/test") else 0
        },
        "test_categories": [
            "å•å…ƒæµ‹è¯• (Unit Tests)",
            "é›†æˆæµ‹è¯• (Integration Tests)", 
            "ç«¯åˆ°ç«¯æµ‹è¯• (E2E Tests)",
            "æ€§èƒ½æµ‹è¯• (Performance Tests)",
            "è´Ÿè½½æµ‹è¯• (Load Tests)"
        ],
        "coverage_areas": [
            "é¡¹ç›®ç®¡ç†æµç¨‹",
            "æ–‡ä»¶ä¸Šä¼ å¤„ç†",
            "OCRæ–‡æ¡£å¤„ç†",
            "CoTæ•°æ®ç”Ÿæˆ",
            "çŸ¥è¯†å›¾è°±æŠ½å–",
            "æ•°æ®å¯¼å‡ºå¯¼å…¥",
            "ç”¨æˆ·ç•Œé¢äº¤äº’",
            "é”™è¯¯å¤„ç†æœºåˆ¶",
            "æ€§èƒ½åŸºå‡†æµ‹è¯•"
        ]
    }
    
    return summary


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª COT Studio MVP æµ‹è¯•éªŒè¯")
    print("=" * 50)
    
    # éªŒè¯æ­¥éª¤
    steps = [
        ("æµ‹è¯•ç›®å½•ç»“æ„", validate_test_structure),
        ("æµ‹è¯•æ–‡ä»¶å†…å®¹", validate_test_content), 
        ("æµ‹è¯•éœ€æ±‚è¦†ç›–", check_test_requirements),
        ("Pythonä¾èµ–æ£€æŸ¥", check_python_imports)
    ]
    
    results = {}
    for step_name, step_func in steps:
        print(f"\nğŸ” {step_name}...")
        try:
            results[step_name] = step_func()
        except Exception as e:
            print(f"âŒ {step_name}å¤±è´¥: {e}")
            results[step_name] = False
    
    # ç”Ÿæˆæ‘˜è¦
    print("\nğŸ“Š æµ‹è¯•éªŒè¯æ‘˜è¦")
    print("=" * 50)
    
    all_passed = all(results.values())
    
    for step_name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{step_name}: {status}")
    
    # æµ‹è¯•ç»Ÿè®¡
    summary = generate_test_summary()
    print(f"\nğŸ“ˆ æµ‹è¯•æ–‡ä»¶ç»Ÿè®¡:")
    for category, count in summary["test_files"].items():
        print(f"  {category}: {count} ä¸ªæ–‡ä»¶")
    
    print(f"\nğŸ“‹ æµ‹è¯•è¦†ç›–é¢†åŸŸ:")
    for area in summary["coverage_areas"]:
        print(f"  â€¢ {area}")
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {'âœ… æ‰€æœ‰éªŒè¯é€šè¿‡' if all_passed else 'âŒ éƒ¨åˆ†éªŒè¯å¤±è´¥'}")
    
    if all_passed:
        print("\nğŸ‰ æµ‹è¯•å¥—ä»¶éªŒè¯å®Œæˆï¼å¯ä»¥å¼€å§‹æ‰§è¡Œæµ‹è¯•ã€‚")
        print("\nğŸ“ è¿è¡Œæµ‹è¯•çš„å‘½ä»¤:")
        print("  åç«¯æµ‹è¯•: python backend/tests/test_runner.py")
        print("  å‰ç«¯æµ‹è¯•: cd frontend && npm run test:run")
        print("  æ€§èƒ½åŸºå‡†: python backend/tests/benchmark.py")
    else:
        print("\nâš ï¸  è¯·ä¿®å¤ä¸Šè¿°é—®é¢˜åé‡æ–°éªŒè¯ã€‚")
    
    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())